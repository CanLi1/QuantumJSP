{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qboost: Binary Classification with Quantum Computer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "D-Wave quantum computer has been widely studied as a discrete optimization engine that accepts any problem formulated as quadratic unconstrained  binary  optimization  (QUBO). In 2008, Google and D-Wave published a paper describing an algorithm to make binary classification amenable to quantum computing, namely the `Qboost` algorithm [Training a Binary Classifier with the Quantum Adiabatic Algorithm](https://arxiv.org/pdf/0811.0416.pdf). \n",
    "\n",
    "In Qboost, the problem is formulated as a thresholded linear superposition of a set of weak classifiers. D-Wave quantum computer is then used to optimize the weights in a learning process that strives to minimize the training error as well as the number of weak classifiers used.\n",
    "\n",
    "\n",
    "Boosting is a category of ensemble method, along with `bagging` and `voting`. An ensemble method is about building an improved model by combining multiple models with the goal of:\n",
    "\n",
    "* decreasing variance (bagging)\n",
    "* decreasing bias (boosting), and\n",
    "* improving prediction (voting)\n",
    "\n",
    "![Boosting Algorithm](images/boosting.jpg)\n",
    "\n",
    "For ensemble method, new training data sets are constantly produced by random sampling with replacement from the original set. The difference is: in _bagging_, any element has the same probability to appear in a new dataset, but in _boosting_, data elements are weighted before they are collected in the new dataset (https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/). Another distinct difference is that bagging is parallelizable but boosting has to be executed sequentially.\n",
    "\n",
    "![Bagging vs Boosting](https://quantdare.com/wp-content/uploads/2016/04/bb2-800x307.png)\n",
    "\n",
    "Voting method operates on labels only. Unlike the boosting method, the aggeragated classification performance will not used to further polish each weak classifiers. Normally, voting method has two requirements: __a large number__ of weak classifiers and a collection of __diversified__ weak classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explain how a Qboost algorithm can be used to solve binary classification problem. \n",
    "\n",
    "First, let us import the packages used in the rest of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "import dwave_micro_client_dimod as micro\n",
    "from sklearn.datasets import fetch_mldata #load_breast_cancer, load_wine\n",
    "from qboost import WeakClassifiers, QBoostClassifier, QboostPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak and strong classifiers\n",
    "As a reference example, we have chosen the following four classifiers:\n",
    "    1. Adaboost\n",
    "    2. Decision Trees\n",
    "    3. Random Forest\n",
    "    4. Qboost\n",
    "However, any commonly used classification models can be use instead. \n",
    "\n",
    "Furthermore, we have embedded another Qboost on top these three classifiers, namely __QboostPlus__.\n",
    "\n",
    "### Adaboost\n",
    "Adaboost combines a number of $N$ weak classifiers into a strong one by\n",
    "$$C(x) = sign\\left(\\sum_i^N w_i c_i(x)\\right),$$\n",
    "with $c_i(x) \\in [-1, +1]$ being the $i$-th weak classifier.\n",
    "\n",
    "$$c_i(x) = sign(w'*x + b)$$\n",
    "\n",
    "The loss function of Adaboost is defined as\n",
    "$$\n",
    "L = \\sum_{n=1}^N \\exp\\left\\{ - y_n \\sum_{s=1}^S w_sc_k(x_n)\\right\\}\n",
    "$$\n",
    "\n",
    "The strong classifier $C(\\cdot)$ is constructed in an iterative fashion. In each iteration, one weak classifer\n",
    "is selected and re-learned to minimize the weighted error function. Then, its weight will be adjusted and renormalized to make sure the sum of all weights equals to 1. \n",
    "\n",
    "The final classification model will be decided by a weighted “vote” of all the weak classifiers. \n",
    "\n",
    "### Decision Trees\n",
    "A decision tree builds on top of tree structure with the non-leaf nodes encoding the decision rules and the leaf nodes encoding the labels. Construction of a decision tree is done by optimizing either entropic or information-theoretic metric. In controlling the depth of a decision tree, we indirectly decide the sub-dimension of the dataset. \n",
    "\n",
    "Since decision trees are simple to construct and also fast to do inference, we normally choose decision trees as the weak classifiers in Adaboost. In `scikit-learn` package, Adaboost is implemented with decision trees of depth 1, also known as _tree stumps_.\n",
    "\n",
    "In our example, we offer an alternative way of implement boosting a number of deeper decision trees.\n",
    "\n",
    "### Qboost\n",
    "To make use of the optimization power of D-Wave quantum annealer, we needs to formulate our objective function as a quadratic unconstrained binary optimization (QUBO) problem. Therefore, we replace the exponential loss as in Adaboost with the following quadratic loss\n",
    "$$\n",
    "w* = \\arg\\min_w\\left(\\sum_s \\left(\\frac{1}{N}\\sum_n^N w_nc_n(x_s) - y_s\\right)^2\\right) + \\lambda ||w||_0,\n",
    "$$\n",
    "where the regularization term is added to enable controlling of weight sparsity.\n",
    "\n",
    "Note in Qboost, the weight vector is binary.\n",
    "\n",
    "Next, let us define the `metric` and `train_model` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the functions required in this example\n",
    "def metric(y, y_pred):\n",
    "    \"\"\"\n",
    "    :param y: true label\n",
    "    :param y_pred: predicted label\n",
    "    :return: metric score\n",
    "    \"\"\"\n",
    "\n",
    "    return metrics.accuracy_score(y, y_pred)\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, lmd):\n",
    "    \"\"\"\n",
    "    :param X_train: training data\n",
    "    :param y_train: training label\n",
    "    :param X_test: testing data\n",
    "    :param y_test: testing label\n",
    "    :param lmd: lambda used in regularization\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # define parameters used in this function\n",
    "    url = 'https://cloud.dwavesys.com/sapi'\n",
    "    token = 'fill_in_your_token'\n",
    "    solver_name = 'fill_in_your_solver'\n",
    "    NUM_READS = 1000\n",
    "    NUM_WEAK_CLASSIFIERS = 30\n",
    "    TREE_DEPTH = 2\n",
    "    DW_PARAMS = {'num_reads': NUM_READS,\n",
    "                 'auto_scale': True,\n",
    "                 'num_spin_reversal_transforms': 10,\n",
    "                 'postprocess': 'optimization',\n",
    "                 }\n",
    "\n",
    "    # define sampler\n",
    "    dwave_sampler = micro.DWaveSampler(solver_name, url, token)\n",
    "    emb_sampler = micro.EmbeddingComposite(dwave_sampler)\n",
    "\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"Train size: %d, Test size: %d\" %(N_train, N_test))\n",
    "    print('Num weak classifiers:', NUM_WEAK_CLASSIFIERS)\n",
    "\n",
    "    # Preprocessing data\n",
    "    imputer = preprocessing.Imputer()\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    normalizer = preprocessing.Normalizer()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_test = normalizer.fit_transform(X_test)\n",
    "\n",
    "    ## Adaboost\n",
    "    print('\\nAdaboost')\n",
    "    clf1 = AdaBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "    clf1.fit(X_train, y_train)\n",
    "    y_train1 = clf1.predict(X_train)\n",
    "    y_test1 = clf1.predict(X_test)\n",
    "#     print(clf1.estimator_weights_)\n",
    "    print('accu (train): %5.2f'%(metric(y_train, y_train1)))\n",
    "    print('accu (test): %5.2f'%(metric(y_test, y_test1)))\n",
    "\n",
    "    # Ensembles of Decision Tree\n",
    "    print('\\nDecision tree')\n",
    "    clf2 = WeakClassifiers(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    y_train2 = clf2.predict(X_train)\n",
    "    y_test2 = clf2.predict(X_test)\n",
    "#     print(clf2.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train2)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test2)))\n",
    "    \n",
    "    # Random forest\n",
    "    print('\\nRandom Forest')\n",
    "    clf3 = RandomForestClassifier(max_depth=TREE_DEPTH, n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    y_train3 = clf3.predict(X_train)\n",
    "    y_test3 = clf3.predict(X_test)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train3)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test3)))\n",
    "\n",
    "    # Qboost\n",
    "    print('\\nQBoost')\n",
    "    clf4 = QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf4.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train4 = clf4.predict(X_train)\n",
    "    y_test4 = clf4.predict(X_test)\n",
    "    print(clf4.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train4)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test4)))\n",
    "\n",
    "    # QboostPlus\n",
    "    print('\\nQBoostPlus')\n",
    "    clf5 = QboostPlus([clf1, clf2, clf3, clf4])\n",
    "    clf5.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train5 = clf5.predict(X_train)\n",
    "    y_test5 = clf5.predict(X_test)\n",
    "    print(clf5.estimator_weights)\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train5)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test5)))\n",
    "\n",
    "    print(\"===========================================================================\")\n",
    "    print(\"Method \\t Adaboost \\t DecisionTree \\t RandomForest \\t Qboost \\t Qboost+\")\n",
    "    print(\"Train\\t %5.2f \\t\\t %5.2f \\t\\t %5.2f \\t\\t %5.2f \\t\\t %5.2f\"% (metric(y_train, y_train1),\n",
    "                                                                         metric(y_train, y_train2),\n",
    "                                                                         metric(y_train, y_train3),\n",
    "                                                                         metric(y_train, y_train4),\n",
    "                                                                         metric(y_train, y_train5),\n",
    "                                                                        ))\n",
    "    print(\"Test\\t %5.2f \\t\\t %5.2f \\t\\t %5.2f \\t\\t %5.2f \\t\\t %5.2f\"% (metric(y_test, y_test1),\n",
    "                                                                       metric(y_test, y_test2),\n",
    "                                                                       metric(y_test, y_test3),\n",
    "                                                                       metric(y_test, y_test4),\n",
    "                                                                       metric(y_test, y_test5)))\n",
    "    print(\"===========================================================================\")\n",
    "    \n",
    "    return [clf1, clf2, clf3, clf4, clf5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "### Example 1: MNIST dataset binary classfication\n",
    "In the following example, we will transform the MNIST dataset (handwritten digits) into a binary classification problem. We assume all digits that are smaller than 5 are labelled as -1 and the rest digits are labelled as +1.\n",
    "\n",
    "First, let us load the MINIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='data')\n",
    "\n",
    "idx_01 = np.where(mnist.target <= 9)[0]\n",
    "np.random.shuffle(idx_01)\n",
    "idx_01 = idx_01[:15000]\n",
    "idx_train = idx_01[:2*len(idx_01)/3]\n",
    "idx_test = idx_01[2*len(idx_01)/3:]\n",
    "\n",
    "X_train = mnist.data[idx_train]\n",
    "X_test = mnist.data[idx_test]\n",
    "\n",
    "y_train = 2*(mnist.target[idx_train] >4) - 1\n",
    "y_test = 2*(mnist.target[idx_test] >4) - 1\n",
    "\n",
    "print(\"Training data size: (%d, %d)\" %(X_train.shape))\n",
    "print(\"Testing data size: (%d, %d)\" %(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the digits, digits with class $+1$ are shown by images with black background and digits with class $-1$  are shown by images with white background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    if y_train[i] == 1:\n",
    "        COLORMAP = 'gray'\n",
    "    else:\n",
    "        COLORMAP = 'gray_r'\n",
    "    plt.subplot(4,4, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap=COLORMAP)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start training the model\n",
    "clfs = train_model(X_train, y_train, X_test, y_test, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## To visualize the decision trees, comment out the following codes\n",
    "# import graphviz\n",
    "# clf = clfs[0]\n",
    "# graph = graphviz.Source(tree.export_graphviz(clf.estimators_[0], out_file=None))\n",
    "# graph.render(None, view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example 2: Statoil/C-CORE Iceberg Classifier Challenge\n",
    "\n",
    "<img src=https://i.imgur.com/q7uAjTM.jpg width=\"300\">\n",
    "Drifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada. The Statoil challenge aims at detecting drifting icebergs (https://www.kaggle.com/c/statoil-iceberg-classifier-challenge) to help reduce the cost of safe working conditions.\n",
    "\n",
    "The classification algorithm needs to predict whether an image contains a ship (labelled at 0) or an iceberg (labelled as 1). The labels are provided by human experts and geographic knowledge on the target. All the images are 75x75 images with two bands, namely `band_1` and `band_2` in the dataset. In the training data, there is another dimension called `inc_angle`, which shows the incidence angle of which the image was taken. Since the testing data size is too big (1.5G), we will only use the training data and keep 1/3 of it as our testing data in this example.\n",
    "\n",
    "First, let us load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statoil data\n",
    "data_train = pd.read_json('data/statoil/train.json')\n",
    "\n",
    "X_raw_df = pd.DataFrame(data_train.band_1.values.tolist())\n",
    "X_raw_df.add(pd.DataFrame(data_train.band_2.values.tolist()))\n",
    "X_raw_df.add(pd.DataFrame(data_train.inc_angle))\n",
    "X_raw = X_raw_df.as_matrix()\n",
    "y_raw = data_train.is_iceberg.as_matrix()\n",
    "y_raw = 2*y_raw - 1\n",
    "\n",
    "data_size = len(X_raw_df)\n",
    "\n",
    "print('Dataset shape: (%d, %d)' %(X_raw.shape))\n",
    "\n",
    "TRAIN_SIZE = int(2.*data_size/3)\n",
    "X_train = X_raw[:TRAIN_SIZE]\n",
    "y_train = y_raw[:TRAIN_SIZE]\n",
    "X_test = X_raw[TRAIN_SIZE:]\n",
    "y_test = y_raw[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "train_model(X_train, y_train, X_test, y_test, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}